{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4, BEE 6940 (Due By 5/4/23, 9:00PM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:\n",
    "\n",
    "**ID**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Problem 1 asks you to conduct several model checks (graphical and quantitative) on a stationary GEV model for tide gauge data.\n",
    "- Problem 2 asks you to do the same on a non-stationary GEV model.\n",
    "- Problem 3 asks you to compare the results of Problems 1 and 2 to draw conclusions about the evidence for non-stationarity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment\n",
    "\n",
    "The following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using StatsBase\n",
    "using CSV # load CSV data\n",
    "using Dates\n",
    "using DataFrames # data storage and presentation\n",
    "using DataFramesMeta\n",
    "using Plots # plotting library\n",
    "using StatsPlots # statistical plotting\n",
    "using Distributions # statistical distribution interface\n",
    "using Turing # probabilistic programming and MCMC\n",
    "using Optim # optimization library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Next, let's load and process the data (as we did in Lab 09). This notebook uses hourly data from the San Francisco, CA tide gauge station, obtained from the [University of Hawaii Sea Level Center](https://uhslc.soest.hawaii.edu/) [(Caldwell et al (2015))](https://doi.org/10.7289/V5V40S7W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data from the file and return a DataFrame of DateTime values and gauge measurements\n",
    "\n",
    "function load_data(fname)\n",
    "    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n",
    "    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n",
    "    df = @chain fname begin\n",
    "        CSV.read(DataFrame; header=false)\n",
    "        rename(\"Column1\" => \"year\", \"Column2\" => \"month\", \"Column3\" => \"day\", \"Column4\" => \"hour\", \"Column5\" => \"gauge\")\n",
    "        # need to reformat the decimal date in the data file\n",
    "        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n",
    "        # replace -99999 with missing\n",
    "        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)\n",
    "        select(:datetime, :gauge)\n",
    "    end\n",
    "    return df\n",
    "end\n",
    "\n",
    "dat = load_data(\"data/h551.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detrend the data to remove the effects of sea-level rise and seasonal dynamics\n",
    "\n",
    "ma_length = 366\n",
    "ma_offset = Int(floor(ma_length/2))\n",
    "moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\n",
    "dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## group data by year and compute the annual maxima\n",
    "\n",
    "dat_ma = dropmissing(dat_ma) # drop missing data\n",
    "dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime => x->year.(x)), :datetime_function))\n",
    "delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\n",
    "rename!(dat_annmax, :datetime_function => :Year)\n",
    "select!(dat_annmax, [:Year, :residual])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems (100 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Stationary Model (40 points)\n",
    "\n",
    "In this problem, you will fit a stationary GEV model and compute the Akaike and Deviance Information Criteria. You will also look at relevant graphical checks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.1: Fit the model (15 points)\n",
    "\n",
    "Construct a `Turing.jl` stationary model with appropriate priors (there's no right choice; remember that thinking about the priors is part of the model checking process). Find the MLE estimate (you'll need this for the AIC) and use MCMC to sample from the posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1.2: Compute information criteria (10 points)\n",
    "\n",
    "Compute AIC and DIC for the stationary model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3: Graphical checks (15 points)\n",
    "\n",
    "Conduct relevant graphical checks you can think of. These could include return periods, credible intervals for the data, or any other statistics that seem appropriate. Explain what each check is for and what your conclusions are about the model based on them. What would you change about the model in 1.1, if anything, based on these checks?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Nonstationary Model (40 points)\n",
    "\n",
    "Next, we will model the tidal extremes using a non-stationary GEV distribution, where the location parameter (but not the shape or scale) is represented by a linear regression $$\\mu_t = \\mu_0 + \\mu_1 x_t,$$ where $x_t$ is the annual mean [Pacific Decadal Oscillation (PDO)](https://climatedataguide.ucar.edu/climate-data/pacific-decadal-oscillation-pdo-definition-and-indices) index, which is based on the variability of sea-surface temperatures (SSTs) in the North Pacific (versus the El Niño–Southern Oscillation (ENSO), which emphasizes the equatorial SSTs).\n",
    "\n",
    "First, let's load the [PDO index dataset from NOAA](https://www.ncei.noaa.gov/access/monitoring/pdo/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data from the file and return a DataFrame of DateTime values and gauge measurements\n",
    "\n",
    "function load_pdo(fname)\n",
    "    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n",
    "    df = CSV.read(fname, DataFrame; delim=\" \", header=2, ignorerepeated=true)\n",
    "    # take yearly average\n",
    "    @transform!(df, :PDO = mean(AsTable(names(df)[2:13])))\n",
    "    @select!(df, $[:Year, :PDO])\n",
    "    @rsubset!(df, :Year != 2023)\n",
    "    return df\n",
    "end\n",
    "\n",
    "pdo = load_pdo(\"data/ersst.v5.pdo.dat\")\n",
    "# subset for years that match the tide gauge data\n",
    "years = dat_annmax[!, :Year]\n",
    "@rsubset!(pdo, :Year in years)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.1: Fit the model (15 points)\n",
    "\n",
    "Construct a `Turing.jl` nonstationary model with appropriate priors (there's no right choice; remember that thinking about the priors is part of the model checking process). Find the MLE estimate (you'll need this for the AIC) and use MCMC to sample from the posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.2: Compute information criteria (10 points)\n",
    "\n",
    "Compute AIC and DIC for the nonstationary model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3: Graphical checks (15 points)\n",
    "\n",
    "Conduct relevant graphical checks you can think of. These could include return periods, credible intervals for the data, or any other statistics that seem appropriate. Explain what each check is for and what your conclusions are about the model based on them. What would you change about the model in 1.1, if anything, based on these checks?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (20 points)\n",
    "\n",
    "Based on the information criteria and your graphical checks, what do you think is the relative evidence for dependence of the San Francisco tide gauge extremes on the PDO? Can you draw a conclusion about which model is better? What else could you try?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
